# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sNgmH38EodL2YIx7eP_QS_hcVuNZU-Vh
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all required libraries

# %matplotlib inline
import itertools
import numpy as np
import pandas as pd
import cv2
import os
import matplotlib.pyplot as plt

from tensorflow.keras.models import load_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, MaxPooling2D
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from google.colab.patches import cv2_imshow

# Mount google drive in colab
from google.colab import drive
drive.mount('/content/drive')

# Url for data.csv in drive
# This file consists of 92000 images of hindi consonants and numbers out of which each letter has approximately 1700 images
path_url = "/content/drive/MyDrive/Mosiac 2K21 Data/data.csv"
path_url_vowels = "/content/drive/MyDrive/Mosiac 2K21 Data/character_vowels.csv"

# Data load
df = pd.read_csv(path_url)
df2 = pd.read_csv(path_url_vowels)

# Append both dataframes
new_cols = {x: y for x, y in zip(df2.columns, df.columns)}
df = df.append(df2.rename(columns=new_cols))

a = ['character_03_ga' , 'character_05_kna' , 'character_09_jha', 'character_10_yna', 'character_15_adna' , 'character_29_waw', 'character_26_yaw', 'character_19_dha' , 'character_18_da', 'character_24_bha' , 'character_30_motosaw' , 'digit_0', 'digit_1', 'digit_2', 'digit_3', 'digit_4', 'digit_5', 'digit_6', 'digit_7', 'digit_8', 'digit_9', 'character_uu', "character_02_kha", "character_07_chha", "character_13_daa", "character_16_tabala", "character_20_na"]
df = df[~df['character'].isin(a)]

# convert pandas dataframe to numpy array
data = np.array(df)

# Make our data random 
np.random.shuffle(data)

# Total number of classes
n_classes = 48

# Seperate X and Y from data
X = data[:, 0:1024]
Y = data[:, 1024]
string = np.unique(Y)

# Normalize the data
X = X / 255

# reshape into 4 rank
X = X.reshape(X.shape[0], 32, 32, 1)

# use labelencoding to convert labels to numbers
label = LabelEncoder()
Y = label.fit_transform(Y)
Y = np_utils.to_categorical(Y)

# Make More data by using ImageGenerator
data_generator_train = ImageDataGenerator( 
    validation_split=0.05, 
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.5,
    zoom_range=0.2
  )

data_generator_valid = ImageDataGenerator(validation_split=0.05)

# # # split our data into train and test to test our model
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.02, random_state=42)

# convert inputs again to numpy array so we don't get any errors
X_train = np.asarray(X_train).astype('float32')
X_test = np.asarray(X_test).astype('float32')

# Sequential Model for detecting hindi letters
Model = Sequential()

Model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))
Model.add(Conv2D(32, kernel_size=(5, 5), activation='relu'))
Model.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=(2,2)))
Model.add(Dropout(0.3))

Model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
Model.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))
Model.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=(2,2)))
Model.add(Dropout(0.3))

Model.add(Flatten())
Model.add(Dense(128, activation='relu', kernel_initializer='uniform'))
Model.add(Dropout(0.3))
Model.add(Dense(64, activation='relu', kernel_initializer='uniform'))
Model.add(Dropout(0.3))
Model.add(Dense(21, activation='softmax', kernel_initializer='uniform'))

Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Data to be fit 
training_data_generator = data_generator_train.flow(X_train, Y_train, batch_size=1025, subset='training')
validation_data_generator = data_generator_valid.flow(X_train, Y_train, batch_size=1025, subset='validation')

# Fitting the model by using 50 iterations
history = Model.fit(training_data_generator, epochs=50, validation_data=validation_data_generator)

# Save the Model
!mkdir -p saved_model
Model.save('saved_model/Mosiac2K21_With_21_characters.h5')

# Plotting our training accuracy and validation accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

# Show testing loss and accuracy
test_loss, test_acc = Model.evaluate(X_train,  Y_train, verbose=2)

# This function is used to calclate how much wrong characters recognised by model
def plot_confusion_matrix(cm, classes,
    normalize=False,
    title='Confusion matrix',
    cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize=(20, 20))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.savefig("Confusion matrix.jpg")

# # function for predicting image
characters = "कखगघङचछजझञटठडढणतथदधनपफबभमयरलवशषसहक्षत्रज्ञॠऊ0१७८४५६७८९"
image_size = 32

def draw(path):
  image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
  cv2_imshow(image)
  gray_img = cv2.resize(image, (image_size, image_size), interpolation = cv2.INTER_AREA)
  gray_img = np.array(gray_img)
  gray_img = gray_img.astype('float32')
  gray_img /= 255 
  gray_img.resize((1, 32, 32, 1))
  pred = Model.predict(gray_img)
  print(np.argmax(pred, axis=1))
  return characters[np.argmax(pred, axis=1)[0]]

# Calculating Predicted test data and draw its confusion Matrix
P = np.asarray([Model.predict(X_test)])
final_pred = np.argmax(np.mean(P,axis=0), axis=1)

cm = confusion_matrix(np.argmax(Y_test, axis=1), final_pred)
strings = np.unique(string)
li = []
for s in range(strings.shape[0]):
  li.append(strings[s])
cm_plot_labels = li
plot_confusion_matrix(cm, cm_plot_labels)

path = "/content/4.jpg"
draw(path)

